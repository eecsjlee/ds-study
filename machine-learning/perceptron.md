# 퍼셉트론 (Perceptron)

퍼셉트론은 인공신경망(Artificial Neural Network)의 가장 기초적인 형태로, 1958년 프랭크 로젠블라트(Frank Rosenblatt)에 의해 제안되었습니다. 입력값의 가중합을 계산한 뒤, 활성화 함수를 통해 출력을 결정하는 간단한 구조를 가지고 있습니다.

## 1. 퍼셉트론의 구조

퍼셉트론은 다음과 같은 구조를 가집니다:

```
입력 x1, x2, ..., xn
↓
가중합 (w1·x1 + w2·x2 + ... + wn·xn + b)
↓
활성화 함수 (예: 계단 함수)
↓
출력 (0 또는 1)
```

### 수식 표현

퍼셉트론의 수학적 표현은 다음과 같습니다:

```
output = activation(Σ(wi·xi) + b)
```

- **xi**: 입력값  
- **wi**: 가중치  
- **b**: 바이어스 (bias)  
- **activation()**: 일반적으로 계단 함수 (step function)

## 2. 활성화 함수 (Activation Function)

퍼셉트론에서는 일반적으로 **계단 함수 (Step Function)** 를 사용합니다:

```
f(z) = 1 if z > 0
\= 0 otherwise
```

하지만 이후의 신경망 구조에서는 다음과 같은 **비선형 활성화 함수**도 사용됩니다:

| 함수 이름 | 수식 | 출력 범위 | 특징 |
|----------|------|------------|--------|
| Sigmoid | 1 / (1 + e^(-x)) | (0, 1) | 출력이 확률처럼 해석 가능 |
| Tanh | (e^x - e^-x) / (e^x + e^-x) | (-1, 1) | 중심성이 좋음, Sigmoid보다 성능 우수 |
| ReLU | max(0, x) | [0, ∞) | 계산 빠르고 성능 우수, 딥러닝에서 널리 사용 |

## 3. 퍼셉트론의 학습

퍼셉트론은 지도 학습 방식으로 학습하며, 예측 결과와 정답(label)의 차이를 이용해 가중치와 바이어스를 업데이트합니다.

### 학습 목표

출력이 정답과 같아지도록 가중치(`wi`)와 바이어스(`b`)를 조정합니다.

### 가중치 업데이트 규칙

```
wi ← wi + α · (y - ŷ) · xi
b  ← b + α · (y - ŷ)
```

- **y**: 실제 값 (label)  
- **ŷ**: 예측 값  
- **α**: 학습률 (learning rate)

## 4. 퍼셉트론의 한계

퍼셉트론은 단층 구조이기 때문에 다음과 같은 한계점이 존재

- **선형 분리만 가능**: 선형적으로 구분되지 않는 문제는 해결 불가능
- 대표적인 예: **XOR 문제**

### 해결 방법

- 여러 층을 쌓은 **다층 퍼셉트론 (MLP)** 으로 확장
- **은닉층(Hidden Layer)** 을 도입하여 비선형 문제 해결 가능
- **오차역전파 (Backpropagation)** 알고리즘을 통해 학습 가능

## 5. 퍼셉트론의 의의

- 가장 단순한 인공신경망 구조이지만, **신경망 이론의 시초**로서 큰 의미가 있음
- 퍼셉트론의 한계를 극복하기 위한 연구가 **딥러닝** 발전의 초석이 됨
- 현대의 인공지능 모델(CNN, RNN 등)도 기본적으로 퍼셉트론 구조를 확장한 것

## 참고 자료

- Rosenblatt, F. (1958). *The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain*
- Ian Goodfellow et al., *Deep Learning*, MIT Press
- Andrew Ng, *Machine Learning*, Coursera
